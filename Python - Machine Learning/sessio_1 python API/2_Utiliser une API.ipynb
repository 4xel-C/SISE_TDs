{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction des données complètes pour le Rhones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "# URL de base de l'API\n",
    "base_url = \"https://data.ademe.fr/data-fair/api/v1/datasets/dpe03existant/lines\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du ficher des adresses du rhones.\n",
    "df69 = pd.read_csv(\"data/adresses-69.csv\", sep=\";\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des codes postaux.\n",
    "code_postaux = df69[\"code_postal\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methode de requête de l'API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_api(params: dict, next_url: str | None = None, retries: int = 3, backoff: int = 2) -> list[dict]:\n",
    "    \"\"\"Récupère toutes les pages d'une requête API en boucle avec gestion des erreurs.\"\"\"\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    recuperated = 0\n",
    "\n",
    "    # Recuperation du code postal des paramètres.\n",
    "    postal_code = params[\"qs\"].split(\":\")[-1]\n",
    "\n",
    "    while True:\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                if next_url:\n",
    "                    response = requests.get(url=next_url)\n",
    "                else:\n",
    "                    response = requests.get(url=base_url, params=params)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "\n",
    "                    content = response.json()\n",
    "\n",
    "                    # Monitoring\n",
    "                    recuperated += content[\"total\"] if content[\"total\"] < params[\"size\"] else params[\"size\"]\n",
    "\n",
    "                    \n",
    "\n",
    "                    if content[\"total\"] == 0:\n",
    "                        print(\"No data found from the query.\", flush=True)\n",
    "                        return []\n",
    "                \n",
    "                    \n",
    "                    all_results.extend(content[\"results\"])\n",
    "                    print(f\"{postal_code}: {recuperated} éléments sur {content[\"total\"]} ({int(recuperated*100/content[\"total\"])}%)\", flush=True)\n",
    "\n",
    "                    next_url = content.get(\"next\")\n",
    "                    break  # sortie du retry loop si succès\n",
    "\n",
    "                # Gestion d'erreur si requête invalide.\n",
    "                else:\n",
    "                    print(f\"{postal_code}: Erreur de connexion avec l'API!, tentative {attempt+1}/{retries}\", flush=True)\n",
    "                    time.sleep(backoff * (attempt + 1))  # backoff progressif\n",
    "\n",
    "            # Gestion d'erreur si problème de connexion.\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"{postal_code}: Erreur réseau: {e}, tentative {attempt+1}/{retries}\", flush=True)\n",
    "                time.sleep(backoff * (attempt + 1))  # backoff progressif\n",
    "\n",
    "        # else de boucle for (si trop d'échecs.)\n",
    "        else:\n",
    "            print(\"{postal_code}: Échec permanent, abandon de cette requête.\")\n",
    "            return all_results # Retourner les informations récupérés jusqu'à présent.\n",
    "\n",
    "        if not next_url:\n",
    "            break\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction normale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = list()\n",
    "\n",
    "for i, code in enumerate(code_postaux):\n",
    "\n",
    "    params = {\n",
    "            \"size\": 10000,   \n",
    "            \"qs\": f\"code_postal_ban:{code}\"\n",
    "        }\n",
    "    df_list.extend(query_api(params=params))\n",
    "\n",
    "# Concaténation de la liste de dictionnaire en dataframe.\n",
    "df = pd.DataFrame(df_list)\n",
    "\n",
    "df.to_csv(\"existants_69.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multitreading code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code finit en erreur, car trop de requêtes simultannées sont demandées à l'API (l'API coupe la connection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_for_code(code):\n",
    "    \"\"\"Exécute query_api pour un code postal donné.\"\"\"\n",
    "    params = {\n",
    "        \"size\": 10000,\n",
    "        \"qs\": f\"code_postal_ban:{code}\"\n",
    "    }\n",
    "    return query_api(params=params, retries=5)\n",
    "\n",
    "\n",
    "# Liste des DataFrames pour tous les codes postaux\n",
    "with ThreadPoolExecutor(max_workers=5) as executor: \n",
    "    all_lists = list(executor.map(fetch_for_code, code_postaux))\n",
    "\n",
    "list_to_df = []\n",
    "for sublist in all_lists:\n",
    "    list_to_df.extend(sublist)\n",
    "\n",
    "# Concaténation finale\n",
    "df_final = pd.DataFrame(list_to_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\axel-\\AppData\\Local\\Temp\\ipykernel_83784\\61002706.py:2: DtypeWarning: Columns (41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"existants_69.csv\", index_col=0)\n"
     ]
    }
   ],
   "source": [
    "# Vérification de l'extraction.\n",
    "df = pd.read_csv(\"existants_69.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batiment neuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliser l'URL suivante: https://data.ademe.fr/data-fair/api/v1/datasets/dpe02neuf/lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération des logements neufs.\n",
    "new_habitation_list = list()\n",
    "\n",
    "base_url = \"https://data.ademe.fr/data-fair/api/v1/datasets/dpe02neuf/lines\"\n",
    "\n",
    "for i, code in enumerate(code_postaux):\n",
    "\n",
    "    params = {\n",
    "            \"size\": 10000,   \n",
    "            \"qs\": f\"code_postal_ban:{code}\"\n",
    "        }\n",
    "\n",
    "    new_habitation_list.extend(query_api(params=params))\n",
    "\n",
    "# Concaténation de la liste de dictionnaire en dataframe.\n",
    "df_news = pd.DataFrame(df_list)\n",
    "\n",
    "# Ecriture du fichier en csv.\n",
    "df_news.to_csv(\"neufs_69.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
